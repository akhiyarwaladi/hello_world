# üîç ULTRATHINK FIGURE EVALUATION & CITATION ANALYSIS

**Generated**: October 8, 2025
**Reviewed**: All 10 main figures + document citations
**Status**: ‚ö†Ô∏è **CRITICAL ISSUES FOUND**

---

## üö® CRITICAL FINDINGS

### **Problem 1: Missing Figure Citations**
- **JICEST Paper**: **0 citations** in body text ‚ùå
- **Laporan Kemajuan**: **1 citation** only ‚ùå

**Impact**: Figures are disconnected from narrative. Readers won't know when/why to look at figures.

### **Problem 2: Figure File Mismatch**
- Document lists: `figure1_sample_images.png`, `figure2_confusion_matrix_species.png`, etc.
- **These files DO NOT EXIST** ‚ùå
- Actual files: `pipeline_architecture.png`, `detection_performance_comparison.png`, etc.

**Impact**: Broken references if trying to insert figures.

---

## üìä INDIVIDUAL FIGURE ANALYSIS

### ‚úÖ **Figure 1: Pipeline Architecture**
**File**: `pipeline_architecture.png` (280 KB)

**Visual Content**:
- Clear flowchart showing Option A architecture
- Input ‚Üí YOLO Detection (3 models) ‚Üí Ground Truth Crops ‚Üí CNN Classification (6 models) ‚Üí Output
- Color-coded stages (blue=YOLO, orange=crops, purple=CNN, green=output)

**Evaluation**:
- ‚úÖ **ESSENTIAL** - This is your core contribution
- ‚úÖ Quality: Excellent, professional diagram
- ‚úÖ Clarity: Very clear, self-explanatory
- ‚úÖ Relevance: **MANDATORY** - explains entire methodology

**Citation Status**: ‚ùå **NOT CITED** in body text

**Where to cite**:
```markdown
Section 2.3 (Methods - Proposed Architecture):
"Our proposed Option A architecture employs a shared classification approach
to achieve computational efficiency, as illustrated in Figure 1."

**Figure 1. Option A Shared Classification Pipeline Architecture**

![Pipeline Architecture](figures/pipeline_architecture.png)

*The pipeline processes blood smear images through three stages: (1) YOLO detection
(v10, v11, v12) generates bounding boxes, (2) ground truth crops are extracted from
annotations once, and (3) six CNN architectures classify parasites. Classification
models are trained once and reused across all detection methods, reducing storage
by 70% and training time by 60%.*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **MUST INCLUDE**

---

### ‚úÖ **Figure 2: Detection Performance Comparison**
**File**: `detection_performance_comparison.png` (339 KB)

**Visual Content**:
- 8 subplots (2 datasets √ó 4 metrics)
- Bar charts comparing YOLO10, YOLO11, YOLO12
- Metrics: mAP@50, mAP@50-95, Precision, Recall
- MP-IDB Species: 92.53-93.12% mAP@50
- MP-IDB Stages: 90.91-92.90% mAP@50

**Evaluation**:
- ‚úÖ **ESSENTIAL** - Primary detection results
- ‚úÖ Quality: Clear, readable, good color coding
- ‚úÖ Clarity: Easy to compare models across metrics
- ‚úÖ Relevance: **MANDATORY** - shows all detection experiments

**Citation Status**: ‚ùå **NOT CITED**

**Where to cite**:
```markdown
Section 3.1 (Results - Detection Performance):
"Detection performance across three YOLO variants and two datasets is
presented in Figure 2."

**Figure 2. YOLO Detection Performance Comparison**

![Detection Performance](figures/detection_performance_comparison.png)

*Comparison of YOLOv10, v11, and v12 across MP-IDB Species and Stages datasets.
All three models achieve competitive performance (mAP@50: 90.91-93.12%), with
YOLOv11 showing the highest recall (92.26% on Species, 90.37% on Stages), making
it optimal for clinical deployment where false negatives are critical.*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **MUST INCLUDE**

---

### ‚úÖ **Figure 3: Classification Heatmap**
**File**: `classification_accuracy_heatmap.png` (260 KB)

**Visual Content**:
- 2√ó6 heatmap (2 datasets √ó 6 models)
- Top row: Accuracy (mostly green, 98-99% for Species, 80-94% for Stages)
- Bottom row: Balanced Accuracy (reveals true performance with imbalance)
- Color scale: Green (high) ‚Üí Orange (medium) ‚Üí Red (low)
- Clear contrast between Species (easier, green) vs Stages (harder, orange/red)

**Evaluation**:
- ‚úÖ **ESSENTIAL** - Shows ALL classification results at a glance
- ‚úÖ Quality: Professional heatmap, good color scheme
- ‚úÖ Clarity: Immediately shows Species > Stages, EfficientNet > ResNet
- ‚úÖ Relevance: **VERY IMPORTANT** - compact visualization of 12 experiments
- ‚ö†Ô∏è **Key insight visible**: Balanced accuracy reveals ResNet50/101 struggle (60-75%)

**Citation Status**: ‚ùå **NOT CITED**

**Where to cite**:
```markdown
Section 3.2 (Results - Classification Performance):
"Classification results for six CNN architectures across two datasets are
summarized in Figure 3."

**Figure 3. Classification Accuracy Heatmap**

![Classification Heatmap](figures/classification_accuracy_heatmap.png)

*Heatmap visualization of classification performance. Top row shows standard accuracy
(98-99% on Species, 80-94% on Stages). Bottom row shows balanced accuracy, revealing
true performance on imbalanced datasets. EfficientNet models (B0, B1) consistently
outperform larger ResNet variants, particularly on balanced accuracy (93.18% vs 75%
for ResNet50 on Species).*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **MUST INCLUDE**

---

### ‚ö†Ô∏è **Figure 4: Training Curves**
**File**: `training_curves.png` (614 KB)

**Visual Content**:
- 4 subplots:
  - YOLO11 Detection Loss (train/val)
  - YOLO11 Detection mAP (train/val + best val line)
  - EfficientNet-B1 Classification Loss
  - EfficientNet-B1 Classification Accuracy
- Shows convergence behavior, slight overfitting gaps

**Evaluation**:
- ‚ö†Ô∏è **OPTIONAL** - Nice to have but not essential
- ‚úÖ Quality: Clear, well-labeled
- ‚ö†Ô∏è Clarity: Requires technical understanding
- ‚ö†Ô∏è Relevance: **SUPPLEMENTARY** - proves models converged properly
- ‚ùå **Limitation**: Only shows 2 models (YOLO11, EfficientNet-B1), not comprehensive

**Citation Status**: ‚ùå **NOT CITED**

**Recommendation**:
- **JICEST Paper**: SKIP (space constraints)
- **Laporan Kemajuan**: INCLUDE in supplementary

**If including**:
```markdown
"Training dynamics demonstrated stable convergence for both detection and
classification models (Figure 4), with validation performance plateauing
after epoch 60 for YOLO and epoch 40 for EfficientNet-B1."
```

**Verdict**: ‚≠ê‚≠ê‚≠ê **OPTIONAL** - Move to supplementary

---

### ‚úÖ **Figure 5: Confusion Matrices**
**File**: `confusion_matrices.png` (224 KB)

**Visual Content**:
- Side-by-side confusion matrices
- Left: Species (EfficientNet-B1, 98.8% accuracy)
  - P. falciparum: 227/227 perfect (100%)
  - Minority classes: Some misclassifications (P. ovale: 5 samples, 3 errors)
- Right: Stages (EfficientNet-B0, 94.31% accuracy)
  - Ring: 265/272 (97.4% correct)
  - Minority classes struggle (Trophozoite: 7/15, Schizont: 5/7, Gametocyte: 2/5)

**Evaluation**:
- ‚úÖ **ESSENTIAL** - Critical for error analysis
- ‚úÖ Quality: Clean, readable numbers
- ‚úÖ Clarity: Shows exactly where models fail
- ‚úÖ Relevance: **MANDATORY** - reveals minority class challenges
- ‚ö†Ô∏è **Key insight**: Confusion mostly among minority classes (Schizont ‚Üî Trophozoite)

**Citation Status**: ‚ùå **NOT CITED**

**Where to cite**:
```markdown
Section 3.2 (Results - Classification Performance):
"Detailed error analysis via confusion matrices (Figure 5) reveals that
misclassifications occur primarily among minority lifecycle stages."

**Figure 5. Confusion Matrices for Best Classification Models**

![Confusion Matrices](figures/confusion_matrices.png)

*Left: Species classification (EfficientNet-B1) achieves perfect 100% accuracy on
P. falciparum (227 samples) but struggles with rare species (P. ovale: 5 samples,
60% accuracy). Right: Stage classification (EfficientNet-B0) shows 97.4% accuracy
on majority class (Ring: 272 samples) but only 40% on minority classes
(Trophozoite: 15, Schizont: 7, Gametocyte: 5 samples).*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **MUST INCLUDE**

---

### ‚úÖ **Figure 6: Species F1-Score Comparison**
**File**: `species_f1_comparison.png` (153 KB)

**Visual Content**:
- Grouped bar chart, 4 species √ó 6 models
- P. falciparum & P. malariae: Perfect 1.0 F1 for all models (flat bars at 100%)
- P. ovale (5 samples): Variable performance (50-77% F1), EfficientNet-B1 best
- P. vivax (18 samples): Consistent 80-87% across models
- Red dashed line at 90% (clinical threshold)

**Evaluation**:
- ‚úÖ **VERY IMPORTANT** - Shows per-class performance breakdown
- ‚úÖ Quality: Clear, color-coded by model
- ‚úÖ Clarity: Immediately shows majority (perfect) vs minority (struggle)
- ‚úÖ Relevance: **CRITICAL** - demonstrates class imbalance impact
- ‚ö†Ô∏è **Key insight**: Only 2/6 models achieve >70% on P. ovale (5 samples)

**Citation Status**: ‚ùå **NOT CITED**

**Where to cite**:
```markdown
Section 3.2 (Results - Per-Class Analysis):
"Per-class F1-scores (Figure 6) demonstrate the severe impact of class imbalance,
with minority species (P. ovale: 5 samples) achieving only 50-77% F1-score
compared to perfect 100% for majority species (P. falciparum: 227 samples)."

**Figure 6. Per-Class F1-Scores for Species Classification**

![Species F1 Comparison](figures/species_f1_comparison.png)

*F1-score comparison across six CNN models for four Plasmodium species. Majority
classes (P. falciparum: 227, P. malariae: 7) achieve perfect 100% F1-scores across
all models. Minority species show variable performance: P. vivax (18 samples)
achieves 80-87%, while P. ovale (5 samples) struggles at 50-77%, with only
EfficientNet-B1 reaching 76.92% via optimized Focal Loss.*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **MUST INCLUDE**

---

### ‚úÖ **Figure 7: Stages F1-Score Comparison**
**File**: `stages_f1_comparison.png` (142 KB)

**Visual Content**:
- Grouped bar chart, 4 lifecycle stages √ó 6 models
- Ring (272 samples): Perfect 97-100% F1 for all models
- Trophozoite (15 samples): Severe drop to 15-52% F1, huge model variance
- Schizont (7 samples): Moderate 63-92% F1, EfficientNet-B0 best
- Gametocyte (5 samples): Consistent 56-75% F1
- Orange dashed line at ~70% (threshold)

**Evaluation**:
- ‚úÖ **VERY IMPORTANT** - Shows worst-case class imbalance (272:5 ratio = 54:1)
- ‚úÖ Quality: Clear visualization
- ‚úÖ Clarity: Dramatic visual contrast between majority (high bars) vs minority (short bars)
- ‚úÖ Relevance: **CRITICAL** - this is your main challenge
- ‚ö†Ô∏è **Key insight**: Trophozoite (15 samples) is hardest class (15-52% F1, confuses with Schizont)

**Citation Status**: ‚ùå **NOT CITED**

**Where to cite**:
```markdown
Section 3.2 (Results - Per-Class Analysis):
"Lifecycle stage classification (Figure 7) reveals the most severe class imbalance
challenge, with Trophozoite (15 samples) achieving only 15-52% F1-score despite
heavy augmentation and Focal Loss optimization."

**Figure 7. Per-Class F1-Scores for Lifecycle Stage Classification**

![Stages F1 Comparison](figures/stages_f1_comparison.png)

*F1-score comparison for four lifecycle stages showing extreme class imbalance impact.
Majority class (Ring: 272 samples) achieves 97-100% F1 across all models. Minority
classes struggle significantly: Trophozoite (15 samples) shows severe degradation
(15-52% F1) with high model variance, Schizont (7 samples) achieves 63-92%, and
Gametocyte (5 samples) maintains 56-75%. The 54:1 imbalance ratio (Ring:Gametocyte)
represents a worst-case scenario for medical imaging.*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **MUST INCLUDE**

---

### ‚úÖ **Figure 8: Class Imbalance Distribution**
**File**: `class_imbalance_distribution.png` (270 KB)

**Visual Content**:
- Two pie charts side-by-side
- Left: Species distribution (Test Set)
  - P. falciparum: 90.8% (227 samples) - dominates entire pie
  - P. vivax: 4.4% (11 samples) - tiny sliver
  - P. malariae: 2.8% (7 samples)
  - P. ovale: 2.0% (5 samples) - barely visible
- Right: Stages distribution (Test Set)
  - Ring: 91.0% (272 samples) - dominates
  - Trophozoite: 5.0% (15 samples)
  - Schizont: 2.3% (7 samples)
  - Gametocyte: 1.7% (5 samples) - barely visible

**Evaluation**:
- ‚úÖ **IMPORTANT** - Powerful visual of the problem
- ‚úÖ Quality: Clear, labeled with counts and percentages
- ‚úÖ Clarity: **INSTANTLY** shows extreme imbalance (91% vs 1-2%)
- ‚úÖ Relevance: **CRITICAL** for Discussion section - explains why minority classes struggle
- ‚ö†Ô∏è **Strength**: Non-technical audiences understand pie charts easily

**Citation Status**: ‚ùå **NOT CITED**

**Where to cite**:
```markdown
Section 4 (Discussion - Class Imbalance Challenge):
"The test set class distribution (Figure 8) reveals extreme imbalance ratios
of 90:1 (P. falciparum vs P. ovale) and 54:1 (Ring vs Gametocyte), representing
a worst-case scenario for malaria classification."

**Figure 8. Class Distribution and Imbalance Analysis**

![Class Imbalance](figures/class_imbalance_distribution.png)

*Test set class distributions for both datasets reveal severe imbalance. Left: Species
classification shows P. falciparum dominance (90.8%, n=227) versus rare species
(P. ovale: 2.0%, n=5). Right: Lifecycle stages exhibit even more extreme imbalance
with Ring (91.0%, n=272) versus Gametocyte (1.7%, n=5), yielding a 54:1 ratio.
This imbalance drives the poor minority class performance observed in Figures 6-7.*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê **STRONGLY RECOMMENDED**

---

### ‚úÖ **Figure 9: Model Efficiency Analysis**
**File**: `model_efficiency_analysis.png` (291 KB)

**Visual Content**:
- Two scatter plots (Species, Stages)
- X-axis: Model parameters (5M - 45M)
- Y-axis: Accuracy (%)
- Each point = one model, labeled
- **Left (Species)**: Shows EfficientNet cluster (5-10M params) at 98.4-98.8% accuracy
  - ResNet50 (25.6M) drops to 98.0%
  - ResNet101 (44.5M) at 98.4% - NO advantage despite 5√ó more params
  - Red dashed line = "Efficiency Frontier" (EfficientNet-B1/DenseNet121)
- **Right (Stages)**: More dramatic
  - EfficientNet-B0 (5.3M) achieves 94.31% (best)
  - ResNet101 (44.5M) at 92.98% - **WORSE** despite being 8√ó larger!
  - EfficientNet-B2 (9.2M) drops to 80.6% (anomaly)

**Evaluation**:
- ‚úÖ **VERY IMPORTANT** - This is a KEY FINDING of your research
- ‚úÖ Quality: Professional scatter plot, clear labeling
- ‚úÖ Clarity: **VISUALLY PROVES** "bigger ‚â† better"
- ‚úÖ Relevance: **CRITICAL CONTRIBUTION** - challenges "deeper is better" paradigm
- ‚ö†Ô∏è **Novelty**: This finding is publication-worthy on its own

**Citation Status**: ‚ùå **NOT CITED**

**Where to cite**:
```markdown
Section 4 (Discussion - Model Efficiency vs Performance):
"A striking finding is that smaller EfficientNet models consistently outperform
larger ResNet variants (Figure 9), challenging the conventional 'deeper is better'
paradigm for small medical imaging datasets."

**Figure 9. Model Parameters vs Classification Accuracy**

![Model Efficiency](figures/model_efficiency_analysis.png)

*Scatter plot revealing inverse relationship between model size and performance on
small datasets. Left: Species classification shows EfficientNet-B0/B1 (5.3-7.8M
parameters) achieving 98.8% accuracy, matching or exceeding ResNet101 (44.5M params,
98.4%) despite 5-8√ó fewer parameters. Right: Stages classification demonstrates
even stronger effect‚ÄîEfficientNet-B0 (5.3M) achieves 94.31%, outperforming ResNet101
(92.98%) despite being 8√ó smaller. The "Efficiency Frontier" (red dashed line)
highlights optimal models. This suggests over-parameterization exacerbates overfitting
on limited training data (<1000 images).*
```

**Verdict**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **MUST INCLUDE** - Key research finding!

---

### ‚ö†Ô∏è **Figure 10: Precision-Recall Tradeoff**
**File**: `precision_recall_tradeoff.png` (239 KB)

**Visual Content**:
- Two scatter plots (Species, Stages)
- X-axis: Recall, Y-axis: Precision
- Each point = one class (for best model only: EfficientNet-B1/B0)
- Diagonal dashed line = "Random Classifier"
- **Left (Species)**:
  - P. falciparum & P. vivax: Top-right corner (100% precision & recall) ‚úÖ
  - P. malariae: (100% recall, 100% precision) ‚úÖ
  - P. ovale: (100% recall, 63% precision) - high recall but low precision
- **Right (Stages)**:
  - Ring & Schizont: Top-right (100% precision & recall) ‚úÖ
  - Gametocyte: (100% recall, 100% precision) ‚úÖ
  - Trophozoite: (~50% recall, ~50% precision) - worst

**Evaluation**:
- ‚ö†Ô∏è **OPTIONAL** - Useful but somewhat redundant
- ‚úÖ Quality: Clear scatter plot
- ‚ö†Ô∏è Clarity: Requires understanding of precision vs recall
- ‚ö†Ô∏è Relevance: **SUPPLEMENTARY** - F1-scores (Figures 6-7) already show this
- ‚ùå **Limitation**: Only shows 1 model per dataset (not comprehensive)
- ‚úÖ **Value**: Shows clinical priority (high recall on P. ovale = good, catches rare cases)

**Citation Status**: ‚ùå **NOT CITED**

**Recommendation**:
- **JICEST Paper**: **SKIP** (redundant with Figures 6-7)
- **Laporan Kemajuan**: INCLUDE in supplementary

**If including**:
```markdown
"Precision-recall analysis (Figure 10) demonstrates that optimized Focal Loss
prioritizes recall over precision for minority classes, achieving 100% recall
on P. ovale (5 samples) despite 63% precision‚Äîa clinically desirable tradeoff
where false negatives (missed rare species) are more critical than false positives."
```

**Verdict**: ‚≠ê‚≠ê **OPTIONAL** - Skip for space constraints

---

## üìä FINAL RECOMMENDATIONS

### **For JICEST Paper (6-8 figures recommended)**

#### ‚úÖ **MANDATORY (6 figures)** - Do NOT remove these:
1. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Figure 1: Pipeline Architecture** - Core contribution
2. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Figure 2: Detection Performance** - Main results
3. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Figure 3: Classification Heatmap** - Results overview
4. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Figure 5: Confusion Matrices** - Error analysis
5. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Figure 6: Species F1-Scores** - Per-class performance
6. ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Figure 9: Model Efficiency** - **KEY RESEARCH FINDING**

#### ‚ö†Ô∏è **STRONGLY RECOMMENDED (+2 figures)**
7. ‚≠ê‚≠ê‚≠ê‚≠ê **Figure 7: Stages F1-Scores** - Shows worst-case imbalance
8. ‚≠ê‚≠ê‚≠ê‚≠ê **Figure 8: Class Imbalance** - Visual problem statement

#### ‚ùå **SKIP for JICEST (move to supplementary)**
9. ‚≠ê‚≠ê‚≠ê **Figure 4: Training Curves** - Nice to have, not essential
10. ‚≠ê‚≠ê **Figure 10: Precision-Recall** - Redundant with F1-scores

**Total for JICEST**: **8 main figures** (6 mandatory + 2 recommended)

---

### **For Laporan Kemajuan (All 10 figures)**

‚úÖ **INCLUDE ALL** - No space constraints, comprehensive report needed

**Main Body**: Figures 1-9
**Supplementary Section**: Figure 10 + all 15 supplementary figures

---

## üö® REQUIRED ACTIONS

### **Action 1: FIX FIGURE FILE REFERENCES** (HIGH PRIORITY)

**Problem**: Document lists non-existent files
```
‚ùå figure1_sample_images.png (DOES NOT EXIST)
‚ùå figure2_confusion_matrix_species.png (DOES NOT EXIST)
... etc
```

**Solution**: Update to actual files
```
‚úÖ pipeline_architecture.png
‚úÖ detection_performance_comparison.png
‚úÖ classification_accuracy_heatmap.png
... etc
```

---

### **Action 2: INSERT FIGURE CITATIONS IN BODY TEXT** (CRITICAL)

#### **JICEST Paper** - Add 8 citations:

**Section 2.3 (Methods):**
```markdown
Our proposed Option A architecture employs a shared classification approach
(Figure 1), where ground truth crops are generated once and reused across
all detection models.
```

**Section 3.1 (Results - Detection):**
```markdown
Detection performance across three YOLO variants is presented in Figure 2,
showing competitive results (mAP@50: 90.91-93.12%).
```

**Section 3.2 (Results - Classification):**
```markdown
Classification results (Figure 3) demonstrate that EfficientNet-B0/B1
consistently outperform larger ResNet models across both datasets.

Confusion matrices (Figure 5) reveal that misclassifications occur primarily
among minority classes with <10 samples.

Per-class analysis (Figures 6-7) quantifies the severe impact of class imbalance,
with minority species/stages achieving only 15-77% F1-scores compared to near-perfect
performance on majority classes.
```

**Section 4 (Discussion):**
```markdown
The test set distribution (Figure 8) exhibits extreme imbalance ratios up to 54:1,
explaining the poor minority class performance.

A striking finding is that smaller models outperform larger ones (Figure 9),
with EfficientNet-B0 (5.3M params) achieving 94.31% versus ResNet101's 92.98%
(44.5M params)‚Äîan 8√ó parameter reduction with better accuracy.
```

#### **Laporan Kemajuan** - Add ~15 citations (Indonesian):

Similar structure, but more detailed since it's a comprehensive report.

---

### **Action 3: ADD FIGURE CAPTIONS** (REQUIRED)

Each figure MUST have:
1. **Figure number and title**
2. **Image insertion** (using markdown/HTML)
3. **Detailed caption** explaining:
   - What is shown
   - Key findings
   - How to interpret

**Template:**
```markdown
**Figure X. [Title]**

![Alt text](figures/filename.png)

*Caption: [2-4 sentences explaining the figure, key findings, and interpretation.
Include specific numbers and highlight important insights.]*
```

---

## üìù CITATION QUALITY CHECKLIST

For each figure citation, ensure:

- [ ] Cited in relevant section (not random placement)
- [ ] Citation explains WHY reader should look at figure
- [ ] Figure number matches actual figure
- [ ] Caption summarizes key takeaway
- [ ] Specific numbers mentioned (not just "shows results")
- [ ] Interpretation provided (not just description)

**Good example:**
```markdown
‚úÖ "Per-class F1-scores (Figure 6) demonstrate severe class imbalance impact,
with minority species (P. ovale: 5 samples) achieving only 50-77% compared
to perfect 100% for P. falciparum (227 samples)."
```

**Bad example:**
```markdown
‚ùå "Results are shown in Figure 6."
‚ùå "See Figure 6 for details."
‚ùå "Figure 6 presents F1-scores."
```

---

## üéØ PRIORITY TASK LIST

### **Immediate (Do First):**
1. ‚úÖ Update figure file list in both documents (fix broken references)
2. ‚úÖ Insert 8 figure citations in JICEST Paper body text
3. ‚úÖ Insert 15+ figure citations in Laporan Kemajuan

### **High Priority:**
4. ‚úÖ Add detailed captions for all 8 JICEST figures
5. ‚úÖ Add detailed captions for all 10 Laporan figures
6. ‚úÖ Verify figure-text integration (citations make sense)

### **Medium Priority:**
7. ‚ö†Ô∏è Consider creating actual figure files if missing (sample images, etc.)
8. ‚ö†Ô∏è Verify supplementary figures are listed correctly

---

## üìä SUMMARY TABLE

| Figure | File | Size | JICEST | Laporan | Priority | Current Citations |
|--------|------|------|--------|---------|----------|-------------------|
| 1. Pipeline | pipeline_architecture.png | 280 KB | ‚úÖ MUST | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Laporan: 1, JICEST: 0 |
| 2. Detection | detection_performance_comparison.png | 339 KB | ‚úÖ MUST | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0 |
| 3. Heatmap | classification_accuracy_heatmap.png | 260 KB | ‚úÖ MUST | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0 |
| 4. Training | training_curves.png | 614 KB | ‚ùå Skip | ‚úÖ Include | ‚≠ê‚≠ê‚≠ê | 0 |
| 5. Confusion | confusion_matrices.png | 224 KB | ‚úÖ MUST | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0 |
| 6. Species F1 | species_f1_comparison.png | 153 KB | ‚úÖ MUST | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0 |
| 7. Stages F1 | stages_f1_comparison.png | 142 KB | ‚úÖ Rec. | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0 |
| 8. Imbalance | class_imbalance_distribution.png | 270 KB | ‚úÖ Rec. | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê | 0 |
| 9. Efficiency | model_efficiency_analysis.png | 291 KB | ‚úÖ MUST | ‚úÖ MUST | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 0 |
| 10. PR Curve | precision_recall_tradeoff.png | 239 KB | ‚ùå Skip | ‚ö†Ô∏è Supp | ‚≠ê‚≠ê | 0 |

**Current Status**:
- JICEST: 0/8 figures cited ‚ùå
- Laporan: 1/10 figures cited ‚ùå

**Target**:
- JICEST: 8/8 figures cited ‚úÖ
- Laporan: 10/10 figures cited ‚úÖ

---

**Generated with**: Claude Code Ultrathink Mode
**Next Step**: Create figure insertion script or manual insertion guide
**Estimated Work**: 2-3 hours for proper citation integration
