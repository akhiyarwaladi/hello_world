#!/usr/bin/env python3
"""
Overfitting Analysis for Malaria Detection Journal Paper
Analyzes relationship between dataset size and model performance
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
import json

class OverfittingAnalyzer:
    def __init__(self):
        self.results_dir = Path("journal_results")
        self.figures_dir = Path("journal_figures")
        self.figures_dir.mkdir(exist_ok=True)

        # Set plotting style
        plt.style.use('default')
        sns.set_palette("husl")

    def create_overfitting_analysis(self):
        """Main function to create overfitting analysis for journal"""

        # Key findings from our results
        experiments_data = {
            'multispecies_overfitted': {
                'dataset_size': 176,
                'train_images': 145,
                'val_images': 31,
                'map50': 0.909,
                'map50_95': 0.530,
                'epochs': 30,
                'analysis': 'severe_overfitting'
            },
            'detection_fixed_realistic': {
                'dataset_size': 103,
                'train_images': 82,  # estimated 80/20 split
                'val_images': 21,
                'map50': 0.738,
                'map50_95': 0.303,
                'epochs': 13,
                'analysis': 'realistic_performance'
            },
            'classification_multispecies': {
                'dataset_size': 1345,  # crops from detection boxes
                'train_images': 1076,
                'val_images': 269,
                'accuracy': 0.967,
                'epochs': 25,
                'analysis': 'balanced_performance'
            }
        }

        # 1. Dataset Size vs Performance Analysis
        self._create_dataset_size_analysis(experiments_data)

        # 2. Learning Curve Analysis
        self._create_learning_curve_analysis()

        # 3. Overfitting Detection Framework
        self._create_overfitting_framework()

        # 4. Statistical Analysis
        self._create_statistical_analysis(experiments_data)

        print("ðŸ“Š Overfitting analysis completed!")
        print(f"ðŸ“ Figures saved to: {self.figures_dir}")

    def _create_dataset_size_analysis(self, experiments_data):
        """Create dataset size vs performance visualization"""

        # Prepare data for plotting
        sizes = []
        performances = []
        types = []
        analyses = []

        for exp_name, data in experiments_data.items():
            if 'map50' in data:
                sizes.append(data['dataset_size'])
                performances.append(data['map50'])
                types.append('Detection')
                analyses.append(data['analysis'])
            elif 'accuracy' in data:
                sizes.append(data['dataset_size'])
                performances.append(data['accuracy'])
                types.append('Classification')
                analyses.append(data['analysis'])

        # Create figure
        plt.figure(figsize=(10, 6))

        # Color map for analysis types
        color_map = {
            'severe_overfitting': 'red',
            'realistic_performance': 'green',
            'balanced_performance': 'blue'
        }

        colors = [color_map[analysis] for analysis in analyses]

        plt.scatter(sizes, performances, c=colors, s=100, alpha=0.7)

        # Add trend line
        z = np.polyfit(sizes, performances, 1)
        p = np.poly1d(z)
        x_trend = np.linspace(min(sizes), max(sizes), 100)
        plt.plot(x_trend, p(x_trend), "k--", alpha=0.5, label='Trend')

        # Annotations
        for i, (size, perf, exp_type, analysis) in enumerate(zip(sizes, performances, types, analyses)):
            plt.annotate(f'{exp_type}\n({size} imgs)', (size, perf),
                        xytext=(10, 10), textcoords='offset points',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7),
                        fontsize=9)

        plt.xlabel('Dataset Size (Images)')
        plt.ylabel('Performance (mAP@0.5 / Accuracy)')
        plt.title('Dataset Size vs Model Performance\nIdentifying Overfitting Patterns')
        plt.grid(True, alpha=0.3)

        # Legend
        legend_elements = [
            plt.scatter([], [], c='red', s=100, label='Severe Overfitting'),
            plt.scatter([], [], c='green', s=100, label='Realistic Performance'),
            plt.scatter([], [], c='blue', s=100, label='Balanced Performance')
        ]
        plt.legend(handles=legend_elements, loc='best')

        plt.tight_layout()
        plt.savefig(self.figures_dir / 'dataset_size_vs_performance.png', dpi=300, bbox_inches='tight')
        plt.savefig(self.figures_dir / 'dataset_size_vs_performance.pdf', bbox_inches='tight')
        plt.close()

    def _create_learning_curve_analysis(self):
        """Analyze learning curves to detect overfitting"""

        # Read the overfitted multispecies results
        multispecies_csv = Path("results/pipeline_final/multispecies_detection_final/results.csv")

        if multispecies_csv.exists():
            df = pd.read_csv(multispecies_csv)

            # Create learning curves
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

            # mAP50 progression
            ax1.plot(df['epoch'], df['metrics/mAP50(B)'], 'b-', linewidth=2, label='Validation mAP@0.5')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('mAP@0.5')
            ax1.set_title('Learning Curve - mAP@0.5 Progression\n(Overfitted Multispecies Dataset)')
            ax1.grid(True, alpha=0.3)
            ax1.legend()

            # Add overfitting indicator
            if df['metrics/mAP50(B)'].iloc[-1] > 0.85:
                ax1.axhline(y=0.85, color='red', linestyle='--', alpha=0.7, label='Overfitting Threshold')
                ax1.text(df['epoch'].iloc[-1] * 0.7, 0.87, 'Likely Overfitting',
                        color='red', fontweight='bold')

            # Loss progression
            ax2.plot(df['epoch'], df['train/box_loss'], 'g-', linewidth=2, label='Training Box Loss')
            ax2.plot(df['epoch'], df['val/box_loss'], 'r-', linewidth=2, label='Validation Box Loss')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Loss')
            ax2.set_title('Training vs Validation Loss\n(Overfitting Detection)')
            ax2.grid(True, alpha=0.3)
            ax2.legend()

            plt.tight_layout()
            plt.savefig(self.figures_dir / 'learning_curves_overfitting.png', dpi=300, bbox_inches='tight')
            plt.savefig(self.figures_dir / 'learning_curves_overfitting.pdf', bbox_inches='tight')
            plt.close()

    def _create_overfitting_framework(self):
        """Create overfitting detection framework visualization"""

        fig, ax = plt.subplots(figsize=(12, 8))

        # Framework components
        framework = {
            'Dataset Size': {
                'Good (>1000)': {'performance': 0.75, 'reliability': 0.9, 'color': 'green'},
                'Acceptable (500-1000)': {'performance': 0.65, 'reliability': 0.75, 'color': 'orange'},
                'Risky (100-500)': {'performance': 0.55, 'reliability': 0.6, 'color': 'yellow'},
                'Dangerous (<100)': {'performance': 0.45, 'reliability': 0.3, 'color': 'red'}
            }
        }

        # Create framework visualization
        categories = ['Good\n(>1000)', 'Acceptable\n(500-1000)', 'Risky\n(100-500)', 'Dangerous\n(<100)']
        performances = [0.75, 0.65, 0.55, 0.45]
        reliabilities = [0.9, 0.75, 0.6, 0.3]
        colors = ['green', 'orange', 'gold', 'red']

        x = np.arange(len(categories))
        width = 0.35

        bars1 = ax.bar(x - width/2, performances, width, label='Expected Performance', color=colors, alpha=0.7)
        bars2 = ax.bar(x + width/2, reliabilities, width, label='Reliability Score', color=colors, alpha=0.4)

        ax.set_xlabel('Dataset Size Category')
        ax.set_ylabel('Score')
        ax.set_title('Overfitting Risk Assessment Framework\nDataset Size vs Expected Performance & Reliability')
        ax.set_xticks(x)
        ax.set_xticklabels(categories)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')

        # Add value labels on bars
        for bar1, bar2, perf, rel in zip(bars1, bars2, performances, reliabilities):
            ax.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.01,
                   f'{perf:.2f}', ha='center', va='bottom', fontweight='bold')
            ax.text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.01,
                   f'{rel:.2f}', ha='center', va='bottom', fontweight='bold')

        # Add our experiments as markers
        ax.scatter([3], [0.91], color='red', s=200, marker='X',
                  label='Multispecies (176 imgs) - OVERFITTED', zorder=10)
        ax.scatter([2.5], [0.74], color='green', s=200, marker='o',
                  label='Detection Fixed (103 imgs) - REALISTIC', zorder=10)

        ax.legend(loc='upper right')

        plt.tight_layout()
        plt.savefig(self.figures_dir / 'overfitting_framework.png', dpi=300, bbox_inches='tight')
        plt.savefig(self.figures_dir / 'overfitting_framework.pdf', bbox_inches='tight')
        plt.close()

    def _create_statistical_analysis(self, experiments_data):
        """Create statistical analysis of overfitting patterns"""

        # Calculate key statistics
        analysis_results = {
            'overfitting_indicators': {
                'multispecies_dataset': {
                    'dataset_size': 176,
                    'val_ratio': 31/176,
                    'performance': 0.909,
                    'overfitting_score': self._calculate_overfitting_score(0.909, 176, 31/176)
                },
                'detection_fixed': {
                    'dataset_size': 103,
                    'val_ratio': 21/103,
                    'performance': 0.738,
                    'overfitting_score': self._calculate_overfitting_score(0.738, 103, 21/103)
                }
            },
            'recommended_thresholds': {
                'min_dataset_size': 500,
                'min_val_ratio': 0.2,
                'max_realistic_map50': 0.85,
                'overfitting_threshold': 0.7
            }
        }

        # Save statistical analysis
        with open(self.figures_dir.parent / 'journal_results' / 'overfitting_analysis.json', 'w') as f:
            json.dump(analysis_results, f, indent=2)

        # Create summary table
        summary_data = []
        for exp_name, data in experiments_data.items():
            overfitting_score = self._calculate_overfitting_score(
                data.get('map50', data.get('accuracy', 0)),
                data['dataset_size'],
                data['val_images'] / data['dataset_size']
            )

            summary_data.append({
                'Experiment': exp_name,
                'Dataset Size': data['dataset_size'],
                'Val Ratio': f"{data['val_images'] / data['dataset_size']:.2f}",
                'Performance': data.get('map50', data.get('accuracy', 0)),
                'Overfitting Score': overfitting_score,
                'Risk Level': self._get_risk_level(overfitting_score),
                'Analysis': data['analysis'].replace('_', ' ').title()
            })

        df_summary = pd.DataFrame(summary_data)
        df_summary.to_csv(self.figures_dir.parent / 'journal_results' / 'overfitting_summary.csv', index=False)

        print("ðŸ“‹ Overfitting analysis summary:")
        print(df_summary.to_string(index=False))

    def _calculate_overfitting_score(self, performance, dataset_size, val_ratio):
        """Calculate overfitting risk score (0-1, higher = more risk)"""

        # Factors contributing to overfitting
        size_factor = max(0, 1 - dataset_size / 1000)  # Penalty for small datasets
        performance_factor = max(0, (performance - 0.85) / 0.15) if performance > 0.85 else 0
        val_ratio_factor = max(0, 1 - val_ratio / 0.2)  # Penalty for small validation set

        overfitting_score = (size_factor + performance_factor + val_ratio_factor) / 3
        return min(1.0, overfitting_score)

    def _get_risk_level(self, overfitting_score):
        """Convert overfitting score to risk level"""
        if overfitting_score > 0.7:
            return 'High Risk'
        elif overfitting_score > 0.4:
            return 'Medium Risk'
        elif overfitting_score > 0.2:
            return 'Low Risk'
        else:
            return 'Minimal Risk'

    def create_publication_figures(self):
        """Create high-quality figures for journal publication"""

        # Set publication quality parameters
        plt.rcParams.update({
            'font.size': 12,
            'axes.titlesize': 14,
            'axes.labelsize': 12,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10,
            'legend.fontsize': 10,
            'figure.figsize': [8, 6],
            'figure.dpi': 300
        })

        self.create_overfitting_analysis()

        print("ðŸŽ¨ Publication-quality figures created!")

def main():
    analyzer = OverfittingAnalyzer()
    analyzer.create_publication_figures()

if __name__ == "__main__":
    main()