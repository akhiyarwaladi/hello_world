#!/usr/bin/env python3
import os
import sys
import json
from pathlib import Path
sys.path.append('/home/akhiyarwaladi/hello_world')

try:
    from ultralytics import YOLO
    import pandas as pd

    # Load model
    model = YOLO("results/current_experiments/training/detection/yolov8_detection/multi_pipeline_20250920_131500_yolo8_det/weights/best.pt")
    print(f"üì¶ Loaded model: YOLO8")

    # Test images directory
    test_dir = Path("data/integrated/yolo/test/images")
    print(f"üñºÔ∏è Test images: {test_dir}")

    # IoU thresholds to test (following reference paper)
    iou_thresholds = [0.3, 0.5, 0.7]

    results_summary = {}

    for iou_thresh in iou_thresholds:
        print(f"\nüéØ Testing IoU threshold: {iou_thresh}")

        # Run validation with specific IoU threshold
        # Note: YOLO uses iou parameter for NMS, but mAP calculation uses fixed thresholds
        # We'll run inference and analyze results at different IoU for NMS
        metrics = model.val(
            data="data/integrated/yolo/data.yaml",
            iou=iou_thresh,
            verbose=False,
            save=False
        )

        # Extract key metrics
        results_summary[f"iou_{iou_thresh:.1f}"] = {
            "iou_threshold": iou_thresh,
            "map50": float(metrics.box.map50),
            "map50_95": float(metrics.box.map),
            "precision": float(metrics.box.mp),
            "recall": float(metrics.box.mr),
            "confidence_threshold": 0.25  # Default YOLO confidence
        }

        print(f"   mAP@0.5: {metrics.box.map50:.3f}")
        print(f"   mAP@0.5:0.95: {metrics.box.map:.3f}")
        print(f"   Precision: {metrics.box.mp:.3f}")
        print(f"   Recall: {metrics.box.mr:.3f}")

    # Save results
    output_file = Path("experiments_organized_20250920_143119/yolo8_complete/04_analysis/iou_variation") / "iou_variation_results.json"
    with open(output_file, 'w') as f:
        json.dump(results_summary, f, indent=2)

    # Create comparison table
    comparison_data = []
    for iou_key, metrics in results_summary.items():
        comparison_data.append({
            "IoU_Threshold": metrics["iou_threshold"],
            "mAP@0.5": f"{metrics['map50']:.3f}",
            "mAP@0.5:0.95": f"{metrics['map50_95']:.3f}",
            "Precision": f"{metrics['precision']:.3f}",
            "Recall": f"{metrics['recall']:.3f}"
        })

    df = pd.DataFrame(comparison_data)
    csv_file = Path("experiments_organized_20250920_143119/yolo8_complete/04_analysis/iou_variation") / "iou_comparison_table.csv"
    df.to_csv(csv_file, index=False)

    # Create markdown report
    md_content = f"""# IoU Variation Analysis - {model_key.upper()}

## üìä Performance at Different IoU Thresholds

Following the reference paper methodology, we test the detection model at different IoU thresholds:

| IoU Threshold | mAP@0.5 | mAP@0.5:0.95 | Precision | Recall |
|---------------|---------|--------------|-----------|--------|
"""

    for data in comparison_data:
        md_content += f"| {data['IoU_Threshold']} | {data['mAP@0.5']} | {data['mAP@0.5:0.95']} | {data['Precision']} | {data['Recall']} |\n"

    md_content += f"""
## üìö Reference Paper Comparison

**Paper Parameters:**
- IoU=0.7 for training
- Testing at IoU: 0.3, 0.5, 0.7
- YOLOv4: mAP@0.5=89% (source), 90% (cross-test)
- YOLOv5: mAP@0.5=96% (source), 59% (cross-test)

**Our Results:**
- Model: {model_key.upper()}
- Best mAP@0.5: {max(results_summary.values(), key=lambda x: x['map50'])['map50']:.3f} at IoU={max(results_summary.values(), key=lambda x: x['map50'])['iou_threshold']}
- Detection confidence threshold: 0.25 (fixed)

## üîß Analysis Notes

- **IoU Threshold**: Controls Non-Maximum Suppression (NMS) overlap tolerance
- **Confidence Threshold**: Fixed at 0.25 for detection filtering
- **Higher IoU**: More strict overlap requirement (fewer detections)
- **Lower IoU**: More lenient overlap (more detections, potential duplicates)

## üìÅ Files Generated

- `iou_variation_results.json`: Raw metrics data
- `iou_comparison_table.csv`: Comparison table
- `iou_analysis_report.md`: This report

---
*Generated by IoU Variation Analysis*
"""

    md_file = Path("experiments_organized_20250920_143119/yolo8_complete/04_analysis/iou_variation") / "iou_analysis_report.md"
    with open(md_file, 'w') as f:
        f.write(md_content)

    print(f"\n‚úÖ IoU variation analysis completed!")
    print(f"üìÅ Results saved to: {Path('experiments_organized_20250920_143119/yolo8_complete/04_analysis/iou_variation')}")
    print(f"üìä Best mAP@0.5: {max(results_summary.values(), key=lambda x: x['map50'])['map50']:.3f} at IoU={max(results_summary.values(), key=lambda x: x['map50'])['iou_threshold']}")

except Exception as e:
    print(f"‚ö†Ô∏è IoU analysis failed: {e}")
    import traceback
    traceback.print_exc()
